/*-------------------------------------------------------------------------
 *
 * nodeReshuffle.c
 *	  Support for reshuffling data in different segments size.
 *
 * DESCRIPTION
 *
 * 		Each table have an `numsegments` attribute in the
 * 		GP_DISTRIBUTION_POLICY table,  it indicate that the table
 * 		data distributed on the first N segments, In common case,
 * 		the `numsegments` equal the total segment count of this
 * 		cluster, that is to say one table need to distributed
 * 		the data on all segments.
 *
 * 		When we add new segments into the cluster, `numsegments` no
 * 		longer equal the actual segment count in the cluster, we
 * 		need to reshuffle the table data to all segments in 2 steps:
 *
 * 			* Reshuffle the table data to all segments
 * 			* Update `numsegments`
 *
 * 		It is easy to update `numsegments`, so we focus on how to
 * 		reshuffle the table data, There are 3 type table in the
 * 		Greenplum database, they are reshuffled in different ways.
 *
 * 		For hash distributed table, we want to reshuffle data
 * 		through Update statement, If we update the hash keys of the
 * 		table, it will generate an Plan like that:
 *
 * 			Update
 * 				->Redistributed Motion
 * 					->SplitUpdate
 * 						->SeqScan
 *
 * 		The SplitUpdate will generate two tuples, one is for
 * 		deleting and another one is for inserting,  one make up by
 * 		old values and another on make up by new values, In the
 * 		first time, SplitUpdate return the deleting tuple to the
 * 		Motion node, Motion node will compute the destination
 * 		segment with the old values, so it can be sent to the
 * 		correct destination segment,  Update node can delete the
 * 		tuple in the destination segment, In the second time,
 * 		SplitUpdate return the inserting table to Motion node,
 * 		Motion node will compute the destination segment with
 * 		the new values, so it also can be sent to the correct
 * 		destination segment, then Update node can insert the
 * 		tuple to the destination segment.
 *
 * 		We can not use this Plan to reshuffle table data directly,
 * 		The problem is that we need to consider the segment count
 * 		when Motion node computing the destination segment. When
 * 		we compute the destination segment of deleting tuple, It
 * 		need use the old segment count which is equal
 * 		`numsegments`, on the other hand, we need use the new
 * 		segment count to compute the destination segment for
 * 		inserting tuple.
 *
 * 		So we can add an new operator Reshuffle to compute the
 * 		destination segment, it record the O and N (O is the count
 * 		of old segments and N is the count of new segments), then
 * 		the Plan would adjust like that:
 *
 * 			Update
 * 				->Explicit Motion
 * 					->Reshuffle
 * 						->SplitUpdate
 * 							->SeqScan
 *
 * 		It can compute the destination segments directly with O and
 * 		N, at the same time we change the Motion type to Explicit,
 * 		it can send tuple to the destination segment which we
 * 		computed in the Reshuffle node.
 *
 * 		With changing the hash method to the `jump hash`, not all
 * 		the table data need to reshuffle, so we add an new
 * 		ReshuffleExpr to filter the tuples which are need to
 * 		reshuffle, this expression will compute the destination
 * 		segment ahead of schedule, if the destination segment is
 * 		current segment, the tuple do not need to reshuffle, with
 * 		the ReshuffleExpr the plan would adjust like that:
 *
 * 			Update
 * 				->Explicit Motion
 * 					->Reshuffle
 * 						->SplitUpdate
 * 							->SeqScan
 * 								|-ReshuffleExpr
 *
 * 		When we want to reshuffle one table, we use the SQL `ALTER
 * 		TABLE xxx SET WITH (RESHUFFLE)`, Actually it will generate
 * 		an new UpdateStmt parse tree, the parse tree is similar to
 * 		the parse tree which is generated by SQL `UPDATE xxx SET
 * 		xxx.aaa = COALESCE(xxx.aaa...) WHERE ReshuffleExpr`. We set
 * 		an reshuffle flag in the UpdateStmt, so it can distinguish
 * 		the common update and the reshuffling.
 *
 * 		In conclusion, we reshuffle hash distributed table by
 * 		Reshuffle node and ReshuffleExpr, the ReshuffleExpr filte
 * 		the tuple need to reshuffle and the Reshuffle node do the
 * 		really reshuffling work, we can use that frame work to
 * 		implement reshuffle random distributed table and replicated
 * 		table.
 *
 * 		For random distributed table, it have no hash keys,  each
 * 		old segment need reshuffle (O - N) / N data to the new
 * 		segments, In the ReshuffleExpr, we can generate a random
 * 		value between [0, N), if the random values is greater than
 * 		O, it means that the tuple need to reshuffle, so SeqScan
 * 		node can return this tuple to ReshuffleNode.  Reshuffle node
 * 		will generate an random value between [O, N), it means which
 * 		new segment the tuple need to insert.
 *
 * 		For replicated table, the table data is same in the all old
 * 		segments, so there do not need to delete any tuples, it only
 * 		need copy the tuple which is in the old segments to the new
 * 		segments, so the ReshuffleExpr do not filte any tuples, In
 * 		the Reshuffle node, we neglect the tuple which is generated
 * 		for deleting, only return the inserting tuple to motion. Let
 * 		me illustrate this with an example.
 *
 * 		if there are 3 old segments in the cluster and we add 4 new
 * 		segments, the segment ID of old segments is (0,1,2), and the
 * 		segment ID of new segments is (3,4,5,6), when reshuffle the
 * 		replicated table, the seg#0 is responsible to copy data to
 * 		seg#3 and seg#6, the seg#1 is responsible to copy data to
 * 		seg#4, the seg#2 is responsible to copy data to seg#5.
 *
 *
 * Portions Copyright (c) 2005-2018, Greenplum inc.
 * Portions Copyright (c) 2012-Present Pivotal Software, Inc.
 * Portions Copyright (c) 1996-2011, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 * IDENTIFICATION
 *	  src/backend/executor/nodeReshuffle.c
 *
 *-------------------------------------------------------------------------
 */

#include "postgres.h"

#include "executor/executor.h"
#include "executor/nodeReshuffle.h"
#include "utils/memutils.h"

#include "cdb/cdbhash.h"
#include "cdb/cdbvars.h"
#include "cdb/memquota.h"

/*
 *  EvalHashSegID
 *
 * 	compute the Hash keys
 */
static int
EvalHashSegID(Datum *values, bool *nulls, List *policyAttrs,
			  List *targetlist, int nsegs)
{
	CdbHash *hnew = makeCdbHash(nsegs);
	uint32 newSeg;
	ListCell *lc;

	Assert(policyAttrs);
	Assert(targetlist);

	cdbhashinit(hnew);

	foreach(lc, policyAttrs)
	{
		AttrNumber attidx = lfirst_int(lc);

		if (nulls[attidx - 1])
		{
			cdbhashnull(hnew);
		}
		else
		{
			TargetEntry *entry = list_nth(targetlist, attidx - 1);
			cdbhash(hnew, values[attidx - 1], exprType((Node *) entry->expr));
		}
	}

	newSeg = cdbhashreduce(hnew);

	return newSeg;

}

/* ----------------------------------------------------------------
 *		ExecReshuffle(node)
 *
 *  For hash distributed tables:
 *  	we compute the destination segment with Hash methods and
 *  	new segments count.
 *
 *  For random distributed tables:
 *  	we get an random value [0, newSeg# - oldSeg#), then the
 *  	destination segment is (random value + oldSeg#)
 *
 *  For replicated tables:
 *  	if there are 3 old segments in the cluster and we add 4
 *  	new segments:
 *  	old segments: 0,1,2
 *  	new segments: 3,4,5,6
 *  	the seg#0 is responsible to copy data to seg#3 and seg#6
 *  	the seg#1 is responsible to copy data to seg#4
 *  	the seg#2 is responsible to copy data to seg#5
 *
 * ----------------------------------------------------------------
 */
TupleTableSlot *
ExecReshuffle(ReshuffleState *node)
{
	PlanState *outerNode = outerPlanState(node);
	Reshuffle *reshuffle = (Reshuffle *) node->ps.plan;
	SplitUpdate *splitUpdate;

	TupleTableSlot *slot = NULL;

	Datum *values;
	bool *nulls;

	int dmlAction;

	Assert(outerNode != NULL);
	Assert(IsA(outerNode->plan, SplitUpdate));

	splitUpdate = (SplitUpdate *) outerNode->plan;

	Assert(splitUpdate->actionColIdx > 0);

	if (reshuffle->ptype == POLICYTYPE_PARTITIONED)
	{
		slot = ExecProcNode(outerNode);

		if (TupIsNull(slot))
		{
			return NULL;
		}

		slot_getallattrs(slot);
		values = slot_get_values(slot);
		nulls = slot_get_isnull(slot);

		dmlAction = DatumGetInt32(values[splitUpdate->actionColIdx - 1]);

		Assert(dmlAction == DML_INSERT || dmlAction == DML_DELETE);

		if (DML_INSERT == dmlAction)
		{
			/* For hash distributed tables*/
			if (NULL != reshuffle->policyAttrs)
			{
				values[reshuffle->tupleSegIdx - 1] =
						Int32GetDatum(EvalHashSegID(values,
													nulls,
													reshuffle->policyAttrs,
													reshuffle->plan.targetlist,
													getgpsegmentCount()));
			}
			else
			{
				/* For random distributed tables*/
				int newSegs = getgpsegmentCount();
				int oldSegs = reshuffle->oldSegs;
				values[reshuffle->tupleSegIdx - 1] =
						Int32GetDatum((random() % (newSegs - oldSegs)) + oldSegs);
			}
		}
#ifdef USE_ASSERT_CHECKING
		else
		{
			if (NULL != reshuffle->policyAttrs)
			{
				Datum oldSegID = values[reshuffle->tupleSegIdx - 1];
				Datum newSegID = Int32GetDatum(
						EvalHashSegID(values,
									  nulls,
									  reshuffle->policyAttrs,
									  reshuffle->plan.targetlist,
									  reshuffle->oldSegs));

				Assert(oldSegID == newSegID);
			}
		}

		/* check */
		if (DatumGetInt32(values[reshuffle->tupleSegIdx - 1]) >=
			getgpsegmentCount())
			elog(ERROR, "ERROR SEGMENT ID : %d",
				 DatumGetInt32(values[reshuffle->tupleSegIdx - 1]));
#endif
	}
	else if (reshuffle->ptype == POLICYTYPE_REPLICATED)
	{
		int segIdx;

		/* For replicated tables*/
		if (GpIdentity.segindex >= reshuffle->oldSegs ||
			GpIdentity.segindex + reshuffle->oldSegs >=
			getgpsegmentCount())
			return NULL;

		/*
		 * Each old semgent cound be responsible to copy data to
		 * more than one new segments
		 */
		do {
			/* To copy data to the first new segments */
			if (node->prevSegIdx == GpIdentity.segindex)
			{
				slot = ExecProcNode(outerNode);
				if (TupIsNull(slot))
				{
					return NULL;
				}

				node->prevSlot = slot;
			}
			else
			{
				/* It seems OK without deep copying the slot*/
				slot = node->prevSlot;
			}

			Assert(!TupIsNull(slot));

			slot_getallattrs(slot);
			values = slot_get_values(slot);

			dmlAction = DatumGetInt32(values[splitUpdate->actionColIdx - 1]);

			Assert(dmlAction == DML_INSERT || dmlAction == DML_DELETE);

			/* Reshuffling replicate table does not need to delete tuple */
			if (dmlAction == DML_DELETE)
				continue;

			/* Get the destination segments */
			segIdx = node->prevSegIdx + reshuffle->oldSegs;
			if (segIdx >= getgpsegmentCount())
			{
				/*
				 * If tuple is copied to all destination segments, we can
				 * process the next tuple now.
				 */
				node->prevSegIdx = GpIdentity.segindex;
				node->prevSlot = NULL;
				continue;
			}

			node->prevSegIdx = segIdx;
			values[reshuffle->tupleSegIdx - 1] = Int32GetDatum(segIdx);

			break;
		} while (1);
	}
	else
	{
		/* Impossible case*/
		Assert(false);
	}

	return slot;
}

/* ----------------------------------------------------------------
 *		ExecInitReshuffle
 *
 * ----------------------------------------------------------------
 */
ReshuffleState *
ExecInitReshuffle(Reshuffle *node, EState *estate, int eflags)
{
	ReshuffleState *reshufflestate;
	bool has_oids;
	TupleDesc tupDesc;

	/* check for unsupported flags */
	Assert(!(eflags & (EXEC_FLAG_MARK | EXEC_FLAG_BACKWARD)) ||
		   outerPlan(node) != NULL);

	/*
	 * create state structure
	 */
	reshufflestate = makeNode(ReshuffleState);
	reshufflestate->ps.plan = (Plan *) node;
	reshufflestate->ps.state = estate;

	/*
	 * initialize child expressions
	 */
	reshufflestate->ps.qual = (List *)
			ExecInitExpr((Expr *) node->plan.qual,
						 (PlanState *) reshufflestate);

	/*
	 * initialize child nodes
	 */
	outerPlanState(reshufflestate) = ExecInitNode(outerPlan(node), estate, eflags);

	/*
	 * we don't use inner plan
	 */
	Assert(innerPlan(node) == NULL);

	/*
	 * tuple table initialization
	 */
	ExecInitResultTupleSlot(estate, &reshufflestate->ps);

	/*
	 * initialize tuple type and projection info
	 */
	ExecAssignResultTypeFromTL(&reshufflestate->ps);
	ExecAssignProjectionInfo(&reshufflestate->ps, NULL);

#if 0
	if (!IsResManagerMemoryPolicyNone()
		&& IsResultMemoryIntensive(node))
	{
		SPI_ReserveMemory(((Plan *)node)->operatorMemKB * 1024L);
	}
#endif

	/* Init the segments id to current segment id */
	reshufflestate->prevSegIdx = GpIdentity.segindex;
	reshufflestate->prevSlot = NULL;

	return reshufflestate;
}

/* ----------------------------------------------------------------
 *		ExecEndReshuffle
 * ----------------------------------------------------------------
 */
void
ExecEndReshuffle(ReshuffleState *node)
{
	/*
	 * Free the exprcontext
	 */
	ExecFreeExprContext(&node->ps);

	/*
	 * clean out the tuple table
	 */
	ExecClearTuple(node->ps.ps_ResultTupleSlot);

	/*
	 * shut down subplans
	 */
	ExecEndNode(outerPlanState(node));

	EndPlanStateGpmonPkt(&node->ps);

	return;
}

/* ----------------------------------------------------------------
 *		ExecReScanReshuffle
 * ----------------------------------------------------------------
 */
void
ExecReScanReshuffle(ReshuffleState *node)
{
	/*
	 * If chgParam of subnode is not null then plan will be re-scanned by
	 * first ExecProcNode.
	 */
	if (node->ps.lefttree &&
		node->ps.lefttree->chgParam == NULL)
		ExecReScan(node->ps.lefttree);

	return;
}
